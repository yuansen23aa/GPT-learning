{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33eb0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a16a5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 4000 # the number of training iterations\n",
    "eval_interval = 200 # how often to evaluate the loss\n",
    "learning_rate = 1e-3 # learning rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200 # number of iterations for evaluation, from random samples\n",
    "embed_size = 64 # embedding size\n",
    "split_ratio = 0.9 # training, val split ratio\n",
    "num_heads = 4\n",
    "num_blocks = 2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bf67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-15 03:32:21--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2026-01-15 03:32:21 (19.0 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058bf2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7b7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01517b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modeldata:\n",
    "    def __init__(self, data, tag):\n",
    "        self.data =data\n",
    "        self.tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d52a067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(split_ratio*len(data)) # first 90% will be train, rest val\n",
    "train_data = modeldata(data[:n], \"train\")\n",
    "val_data = modeldata(data[n:], \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95308619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloaderlite:\n",
    "    def __init__(self, data, block_size, batch_size, shuffle=True, tag=None):\n",
    "        self.data = data\n",
    "        self.L = block_size\n",
    "        self.B= batch_size\n",
    "        self.current_pos = 0\n",
    "        self.shuffle = shuffle\n",
    "        self.N = len(data)\n",
    "        self.tag = tag\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.shuffle:\n",
    "            # shuffle mode: sample random batches\n",
    "            idx = torch.randint(0,self.N - self.L - 1, (self.B,))\n",
    "        else:\n",
    "            # sequential mode: get batches in order, mainly for eval\n",
    "            if self.current_pos + self.B * self.L >= self.N:\n",
    "                self.current_pos = 0  # reset pointer if we reach the end\n",
    "                raise StopIteration\n",
    "            idx = torch.arange(self.current_pos, self.current_pos + self.B * self.L, self.L)\n",
    "            self.current_pos += self.B * self.L\n",
    "        # x is from i -> i + L token index\n",
    "        # y is from 1+1 -> i+ 1 + L token index    \n",
    "        x = torch.stack([self.data[i:i+self.L] for i in idx])\n",
    "        y = torch.stack([self.data[i+1:i+self.L+1] for i in idx])    \n",
    "        return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3a22480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.FFN = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x_id_matrix, y_id_matrix=None):\n",
    "        # id_matrix: (B, L) is the input token indices for B batches, each batch has sequence length L\n",
    "        # d: the embed size\n",
    "        # v: the vocab size\n",
    "        B, L = x_id_matrix.shape\n",
    "        token_emb = self.embed_table(x_id_matrix) # (B, L, d)\n",
    "        logits = self.FFN(token_emb) # (B, L, vocab_size)\n",
    "        if y_id_matrix is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, L, v = logits.shape\n",
    "            # reshape the logits and targets to compute the cross-entropy loss\n",
    "            logits = logits.view(B*L, v)\n",
    "            targets = y_id_matrix.view(B*L)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x_id_matrix, max_new_tokens):\n",
    "        B, L = x_id_matrix.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(x_id_matrix)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            # sample from the distribution\n",
    "            next_id = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x_id_matrix= torch.cat((x_id_matrix, next_id), dim=1) # (B, L+1)\n",
    "        return x_id_matrix\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7f01875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaderlite(train_data.data, block_size, batch_size, shuffle=True, tag=\"train\")\n",
    "val_loader = dataloaderlite(val_data.data, block_size, batch_size, shuffle=False, tag=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "39784258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(vocab_size, embed_size*10)\n",
    "model = model.to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "24b78755",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def loss_estimation(model):\n",
    "    output = {}\n",
    "    model.eval()\n",
    "    train_data_loader_estimation = dataloaderlite(train_data.data, block_size, batch_size, shuffle=True, tag=\"train\")\n",
    "    val_data_loader_estimation = dataloaderlite(val_data.data, block_size, batch_size, shuffle=True, tag=\"val\")\n",
    "    for data_obj in [train_data_loader_estimation, val_data_loader_estimation]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = data_obj.__next__()\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[data_obj.tag] = losses.mean().item()\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c32922a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2918, val loss 4.2892\n",
      "step 200: train loss 2.5473, val loss 2.5620\n",
      "step 400: train loss 2.5312, val loss 2.5513\n",
      "step 600: train loss 2.5156, val loss 2.5495\n",
      "step 800: train loss 2.5242, val loss 2.5537\n",
      "step 1000: train loss 2.5208, val loss 2.5606\n",
      "step 1200: train loss 2.5205, val loss 2.5494\n",
      "step 1400: train loss 2.5030, val loss 2.5575\n",
      "step 1600: train loss 2.5156, val loss 2.5444\n",
      "step 1800: train loss 2.5074, val loss 2.5353\n",
      "step 2000: train loss 2.5052, val loss 2.5242\n",
      "step 2200: train loss 2.5157, val loss 2.5446\n",
      "step 2400: train loss 2.5185, val loss 2.5273\n",
      "step 2600: train loss 2.5041, val loss 2.5381\n",
      "step 2800: train loss 2.5068, val loss 2.5328\n",
      "step 3000: train loss 2.5003, val loss 2.5486\n",
      "step 3200: train loss 2.5117, val loss 2.5363\n",
      "step 3400: train loss 2.5032, val loss 2.5392\n",
      "step 3600: train loss 2.4977, val loss 2.5387\n",
      "step 3800: train loss 2.4933, val loss 2.5294\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = loss_estimation(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = train_loader.__next__()\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c8aeb459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'd payou ead me\n",
      "BAThoweve,\n",
      "Tof s, qurarore gerilyrsondghys R blere ar urss bld u d wachaigureap anuneilf an ppirear a g th nof nd wicowhy:\n",
      "Byowindiveatirw'su quenglle V: wha pe ghat wsod fut'd f aphay:\n",
      "Lowilichesor us s CO t:--lerand oof giearelos s'd alal me was\n",
      "Thome n forind Talingnen at n ubu sse pr s whee tate's st fumy t s, se wahfongisow,\n",
      "ARD:\n",
      "Coomo, geind,\n",
      "Fire\n",
      "ARCImowir\n",
      "\n",
      "ARAng;\n",
      "Yond aleed y,\n",
      "ARDo wnd by beake,\n",
      "THangaithilond y, thenct t he, wize isard oures:\n",
      "MEESerien\n",
      "GLusel ty, rak wead IFOf hichilt plearondet ond we, ty,\n",
      "Frre w! ar bar tis\n",
      "Nowind nors w mifu blld.\n",
      "War bl cis Tones en I cind ILAn hy, hory ly, mactr w blt wat ave te ETrs:\n",
      "Forise.\n",
      "ARKis-myhu ff nonouns f omath'l g CimbeEsos pule baurloyo muronderalite CHAHeril nonct t t, yot lodghermesesend ha VEvest ausalararse fe pe weld\n",
      "Ind?\n",
      "Andeararenofo G testes D:\n",
      "m wis sd sers pe, g heptissof forsouglit:\n",
      "Fovereden s bilshet izeis.\n",
      "Why,'Fr, balowarard ink uourd\n",
      "S: led im'd heas thonseso waran windonoflinn IRIUSTh s! ly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_context_ids = model.generate(context, max_new_tokens=1000)\n",
    "print(decode(generated_context_ids[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e9d54b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, block_size, n_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, embed_size)\n",
    "        self.attention_blocks = nn.ModuleList(MultiHeadAttention(num_heads, embed_size, block_size, batch_size)\n",
    "                                               for _ in range(n_layers))\n",
    "        self.FFN = nn.Linear(embed_size, 4*embed_size)\n",
    "        self.LM = nn.Linear(4*embed_size, vocab_size)\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.block_size = block_size\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def forward(self, x_id_matrix, y_id_matrix=None):\n",
    "        B, L = x_id_matrix.shape\n",
    "        token_emb = self.token_emb_table(x_id_matrix)\n",
    "        # it's very important to use the current sequence length L here\n",
    "        pos_emb = self.pos_emb_table(torch.arange(L))\n",
    "        x = token_emb + pos_emb\n",
    "        for block in self.attention_blocks:\n",
    "          x = x + block(x)\n",
    "        # LM head\n",
    "        logits = self.LM(self.FFN(self.ln(x)))\n",
    "        if y_id_matrix == None:\n",
    "            loss = 0.0 \n",
    "        else:      \n",
    "            targets = y_id_matrix.view(B*L)\n",
    "            logits = logits.view(B*L, self.vocab_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x_id_matrix, max_new_tokens):\n",
    "        B, L = x_id_matrix.shape\n",
    "        out = x_id_matrix.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if x_id_matrix.shape[1] > self.block_size:\n",
    "                x_id_matrix = x_id_matrix[:, -self.block_size:] \n",
    "            \n",
    "            logits, _ = self(x_id_matrix)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            top_k_logits, top_k_indices = torch.topk(logits, k=5)\n",
    "            top_k_probs = F.softmax(top_k_logits, dim=-1) # (B, vocab_size)\n",
    "            # sample from the distribution\n",
    "            sampled = torch.multinomial(top_k_probs, num_samples=1) # (B, 1)\n",
    "            next_id = torch.gather(top_k_indices, -1, sampled)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x_id_matrix= torch.cat((x_id_matrix, next_id), dim=1) # (B, L+1)\n",
    "            out = torch.cat((out, next_id), dim=1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f2e789be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, block_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = nn.ModuleList(MaskedAttention(num_heads, embed_size, block_size, batch_size) \n",
    "                                             for _ in range(num_heads)) \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        chuncks = []\n",
    "        for attention_head in self.attention_heads: \n",
    "            out = attention_head(x) # (B, L, d/H)\n",
    "            chuncks.append(out)\n",
    "        final_out = torch.cat(chuncks,dim=-1)\n",
    "        return final_out # (B, L, d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fd9b92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, block_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.head_size = int(embed_size/num_heads)\n",
    "        self.q_proj = nn.Linear(embed_size, self.head_size, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_size, self.head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_size, self.head_size, bias=False)\n",
    "        self.ff1 = nn.Linear(self.head_size, 4*self.head_size)\n",
    "        self.ff2 = nn.Linear(4*self.head_size, self.head_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(self.head_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.register_buffer('mask',torch.tril(torch.ones(batch_size, block_size, block_size, device=device,requires_grad=False)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, L, d/H)\n",
    "        B, L, _ = x.shape\n",
    "        q, k ,v = self.q_proj(self.ln1(x)), self.k_proj(self.ln1(x)), self.v_proj(self.ln1(x))\n",
    "        kt = k.transpose(-1,-2)\n",
    "        attention_matrix = F.softmax(q@kt, dim=-1)\n",
    "        # (B, L, L)\n",
    "        #attention_matrix[self.mask == 0] = -torch.inf\n",
    "        mask = torch.tril(torch.ones(B, L, L, device=device,requires_grad=False))\n",
    "        attention_matrix = attention_matrix.masked_fill(mask == 0, float('-inf'))\n",
    "        weights = F.softmax(attention_matrix, dim=-1)\n",
    "        attention_output = weights@v\n",
    "        #residual connection\n",
    "        attention_output = q + attention_output\n",
    "        # second layer norm\n",
    "        attention_output = self.ln2(attention_output)\n",
    "        output = self.ff2(self.relu(self.ff1(attention_output)))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a38b53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt = GPT(vocab_size, embed_size, block_size, num_blocks, num_heads)\n",
    "model_gpt = model_gpt.to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer_gpt = torch.optim.AdamW(model_gpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c98ef9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaderlite(train_data.data, block_size, batch_size, shuffle=True, tag=\"train\")\n",
    "val_loader = dataloaderlite(val_data.data, block_size, batch_size, shuffle=True, tag=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3e2e20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2207, val loss 4.2222\n",
      "step 200: train loss 2.4596, val loss 2.4739\n",
      "step 400: train loss 2.2751, val loss 2.2951\n",
      "step 600: train loss 2.1291, val loss 2.1488\n",
      "step 800: train loss 1.8383, val loss 1.8673\n",
      "step 1000: train loss 1.4783, val loss 1.5308\n",
      "step 1200: train loss 1.1388, val loss 1.1748\n",
      "step 1400: train loss 0.9344, val loss 0.9632\n",
      "step 1600: train loss 0.8468, val loss 0.8788\n",
      "step 1800: train loss 0.7988, val loss 0.8303\n",
      "step 2000: train loss 0.7571, val loss 0.7814\n",
      "step 2200: train loss 0.7285, val loss 0.7551\n",
      "step 2400: train loss 0.6971, val loss 0.7278\n",
      "step 2600: train loss 0.6968, val loss 0.7159\n",
      "step 2800: train loss 0.6797, val loss 0.7001\n",
      "step 3000: train loss 0.6655, val loss 0.6845\n",
      "step 3200: train loss 0.6562, val loss 0.6682\n",
      "step 3400: train loss 0.6411, val loss 0.6580\n",
      "step 3600: train loss 0.6424, val loss 0.6664\n",
      "step 3800: train loss 0.6531, val loss 0.6824\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = loss_estimation(model_gpt)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = train_loader.__next__()\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits_gpt, loss_gpt = model_gpt(xb, yb)\n",
    "    optimizer_gpt.zero_grad(set_to_none=True)\n",
    "    loss_gpt.backward()\n",
    "    optimizer_gpt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "302c7a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acqppppithise buth so but wow wand ser whor to wing wird han, bus me mers shave ta shy, the tath sor sto meant shatt sand so to wno what shin wir son myour then mus thy sat my som whattingh o that mon bund what bun bes, wart she ward the then thatth wily with wer tord thand shot must me sardont what wir,\n",
      "Mas thert this shas stis start at the thas,\n",
      "\n",
      "LAN LLAR:\n",
      "Thor sove we sors munt we me sus, thy sord werd, whath wer thear bur therd,\n",
      "To thy sutt ma sto wis sours\n",
      "\n",
      "The mats shats my my hard,\n",
      "Hur gurt the warts mant thers the that thester, the wald mas sang thes beastst of gruth mang mond tand by mous mans tand,\n",
      "Att mentste,\n",
      "Me wom bat, thand buth matt wirs stand thes she may sart berst ward hill band stast bes my wond war mast stoust will st win will, whe wors thengs sove wort som to be shand.\n",
      "Sest thand but the thir bus wird than will me gret town well wh wond that whor bunth,\n",
      "\n",
      "Mut sprand ther stise tour bert, thand mur thy the mat the thas this somert to sum mant, ses, wird thars, and w\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_context_ids = model_gpt.generate(context, max_new_tokens=1000)\n",
    "print(decode(generated_context_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8941f26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8385\n"
     ]
    }
   ],
   "source": [
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_trainable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
