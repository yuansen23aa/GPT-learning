{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33eb0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492ed6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16a5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 4000 # the number of training iterations\n",
    "eval_interval = 200 # how often to evaluate the loss\n",
    "learning_rate = 1e-3 # learning rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200 # number of iterations for evaluation, from random samples\n",
    "embed_size = 64 # embedding size\n",
    "split_ratio = 0.9 # training, val split ratio\n",
    "num_heads = 4\n",
    "num_blocks = 2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13bf67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-15 17:49:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2026-01-15 17:49:43 (28.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058bf2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7b7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f01517b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modeldata:\n",
    "    def __init__(self, data, tag, split_ratio):\n",
    "        self.data =data\n",
    "        self.tag = tag\n",
    "        self.split_ratio = split_ratio\n",
    "    \n",
    "    def __getdata__(self):\n",
    "        n = int(self.split_ratio * len(self.data))\n",
    "        if self.tag == 'train':\n",
    "            out = self.data[:n]\n",
    "        if self.tag == 'val':\n",
    "            out = self.data[n:]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d52a067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data = modeldata(data, \"train\", split_ratio).__getdata__()\n",
    "val_data = modeldata(data, \"val\", split_ratio).__getdata__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7085bbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95308619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloaderlite:\n",
    "    def __init__(self, data, block_size, batch_size, device, shuffle=True, tag=None):\n",
    "        self.data = data\n",
    "        self.L = block_size\n",
    "        self.B= batch_size\n",
    "        self.current_pos = 0\n",
    "        self.shuffle = shuffle\n",
    "        self.device = device\n",
    "        self.N = len(data)\n",
    "        self.tag = tag\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.shuffle:\n",
    "            # shuffle mode: sample random batches\n",
    "            idx = torch.randint(0,self.N - self.L - 1, (self.B,))\n",
    "        else:\n",
    "            # sequential mode: get batches in order, mainly for eval\n",
    "            if self.current_pos + self.B * self.L >= self.N:\n",
    "                self.current_pos = 0  # reset pointer if we reach the end\n",
    "                raise StopIteration\n",
    "            idx = torch.arange(self.current_pos, self.current_pos + self.B * self.L, self.L)\n",
    "            self.current_pos += self.B * self.L\n",
    "        # x is from i -> i + L token index\n",
    "        # y is from 1+1 -> i+ 1 + L token index    \n",
    "        x = torch.stack([self.data[i:i+self.L] for i in idx])\n",
    "        y = torch.stack([self.data[i+1:i+self.L+1] for i in idx])    \n",
    "        return x.to(self.device), y.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a22480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.FFN = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x_id_matrix, y_id_matrix=None):\n",
    "        # id_matrix: (B, L) is the input token indices for B batches, each batch has sequence length L\n",
    "        # d: the embed size\n",
    "        # v: the vocab size\n",
    "        B, L = x_id_matrix.shape\n",
    "        token_emb = self.embed_table(x_id_matrix) # (B, L, d)\n",
    "        logits = self.FFN(token_emb) # (B, L, vocab_size)\n",
    "        if y_id_matrix is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, L, v = logits.shape\n",
    "            # reshape the logits and targets to compute the cross-entropy loss\n",
    "            logits = logits.view(B*L, v)\n",
    "            targets = y_id_matrix.view(B*L)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x_id_matrix, max_new_tokens):\n",
    "        B, L = x_id_matrix.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(x_id_matrix)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            # sample from the distribution\n",
    "            next_id = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x_id_matrix= torch.cat((x_id_matrix, next_id), dim=1) # (B, L+1)\n",
    "        return x_id_matrix\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7f01875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaderlite(train_data, block_size, batch_size, device, shuffle=True, tag=\"train\")\n",
    "val_loader = dataloaderlite(val_data, block_size, batch_size, device, shuffle=False, tag=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39784258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(vocab_size, embed_size*10)\n",
    "model = model.to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24b78755",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def loss_estimation(model):\n",
    "    output = {}\n",
    "    model.eval()\n",
    "    train_data_loader_estimation = dataloaderlite(train_data, block_size, batch_size, device, shuffle=True, tag=\"train\")\n",
    "    val_data_loader_estimation = dataloaderlite(val_data, block_size, batch_size, device, shuffle=True, tag=\"val\")\n",
    "    for data_obj in [train_data_loader_estimation, val_data_loader_estimation]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = data_obj.__next__()\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[data_obj.tag] = losses.mean().item()\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c32922a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2993, val loss 4.2958\n",
      "step 200: train loss 2.5426, val loss 2.5603\n",
      "step 400: train loss 2.5241, val loss 2.5500\n",
      "step 600: train loss 2.5206, val loss 2.5442\n",
      "step 800: train loss 2.5239, val loss 2.5436\n",
      "step 1000: train loss 2.5143, val loss 2.5406\n",
      "step 1200: train loss 2.5109, val loss 2.5488\n",
      "step 1400: train loss 2.4985, val loss 2.5392\n",
      "step 1600: train loss 2.4938, val loss 2.5229\n",
      "step 1800: train loss 2.4978, val loss 2.5414\n",
      "step 2000: train loss 2.5007, val loss 2.5261\n",
      "step 2200: train loss 2.5092, val loss 2.5386\n",
      "step 2400: train loss 2.5006, val loss 2.5391\n",
      "step 2600: train loss 2.4934, val loss 2.5383\n",
      "step 2800: train loss 2.5043, val loss 2.5251\n",
      "step 3000: train loss 2.5182, val loss 2.5399\n",
      "step 3200: train loss 2.5025, val loss 2.5334\n",
      "step 3400: train loss 2.4937, val loss 2.5214\n",
      "step 3600: train loss 2.5083, val loss 2.5467\n",
      "step 3800: train loss 2.5064, val loss 2.5343\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = loss_estimation(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = train_loader.__next__()\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c8aeb459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'd payou ead me\n",
      "BAThoweve,\n",
      "Tof s, qurarore gerilyrsondghys R blere ar urss bld u d wachaigureap anuneilf an ppirear a g th nof nd wicowhy:\n",
      "Byowindiveatirw'su quenglle V: wha pe ghat wsod fut'd f aphay:\n",
      "Lowilichesor us s CO t:--lerand oof giearelos s'd alal me was\n",
      "Thome n forind Talingnen at n ubu sse pr s whee tate's st fumy t s, se wahfongisow,\n",
      "ARD:\n",
      "Coomo, geind,\n",
      "Fire\n",
      "ARCImowir\n",
      "\n",
      "ARAng;\n",
      "Yond aleed y,\n",
      "ARDo wnd by beake,\n",
      "THangaithilond y, thenct t he, wize isard oures:\n",
      "MEESerien\n",
      "GLusel ty, rak wead IFOf hichilt plearondet ond we, ty,\n",
      "Frre w! ar bar tis\n",
      "Nowind nors w mifu blld.\n",
      "War bl cis Tones en I cind ILAn hy, hory ly, mactr w blt wat ave te ETrs:\n",
      "Forise.\n",
      "ARKis-myhu ff nonouns f omath'l g CimbeEsos pule baurloyo muronderalite CHAHeril nonct t t, yot lodghermesesend ha VEvest ausalararse fe pe weld\n",
      "Ind?\n",
      "Andeararenofo G testes D:\n",
      "m wis sd sers pe, g heptissof forsouglit:\n",
      "Fovereden s bilshet izeis.\n",
      "Why,'Fr, balowarard ink uourd\n",
      "S: led im'd heas thonseso waran windonoflinn IRIUSTh s! ly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_context_ids = model.generate(context, max_new_tokens=1000)\n",
    "print(decode(generated_context_ids[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9d54b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, block_size, n_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_emb_table = nn.Embedding(block_size, embed_size)\n",
    "        self.attention_blocks = nn.ModuleList(Block(num_heads, embed_size, block_size, batch_size)\n",
    "                                               for _ in range(n_layers))\n",
    "        self.ff = nn.Linear(embed_size, 4*embed_size)\n",
    "        self.LM = nn.Linear(4*embed_size, vocab_size)\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.block_size = block_size\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def forward(self, x_id_matrix, y_id_matrix=None):\n",
    "        B, L = x_id_matrix.shape\n",
    "        token_emb = self.token_emb_table(x_id_matrix)\n",
    "        # it's very important to use the current sequence length L here\n",
    "        pos = torch.arange(L, device=x_id_matrix.device)\n",
    "        pos_emb = self.pos_emb_table(pos)\n",
    "        x = token_emb + pos_emb\n",
    "        for block in self.attention_blocks:\n",
    "          x = block(x)\n",
    "        # LM head\n",
    "        logits = self.LM(self.ff(self.ln(x)))\n",
    "        if y_id_matrix == None:\n",
    "            loss = 0.0 \n",
    "        else:      \n",
    "            targets = y_id_matrix.view(B*L)\n",
    "            logits = logits.view(B*L, self.vocab_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x_id_matrix, max_new_tokens, top_k):\n",
    "        B, L = x_id_matrix.shape\n",
    "        out = x_id_matrix.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if x_id_matrix.shape[1] > self.block_size:\n",
    "                x_id_matrix = x_id_matrix[:, -self.block_size:] \n",
    "            \n",
    "            logits, _ = self(x_id_matrix)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            top_k_logits, top_k_indices = torch.topk(logits, k=top_k)\n",
    "            top_k_probs = F.softmax(top_k_logits, dim=-1) # (B, vocab_size)\n",
    "            # sample from the distribution\n",
    "            sampled = torch.multinomial(top_k_probs, num_samples=1) # (B, 1)\n",
    "            next_id = torch.gather(top_k_indices, -1, sampled)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x_id_matrix= torch.cat((x_id_matrix, next_id), dim=1) # (B, L+1)\n",
    "            out = torch.cat((out, next_id), dim=1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2e789be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, block_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = nn.ModuleList(MaskedAttention(num_heads, embed_size, block_size, batch_size) \n",
    "                                             for _ in range(num_heads)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        chuncks = []\n",
    "        for attention_head in self.attention_heads: \n",
    "            out = attention_head(x) # (B, L, d/H)\n",
    "            chuncks.append(out)\n",
    "        final_out = torch.cat(chuncks,dim=-1)\n",
    "        return final_out # (B, L, d)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de40d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_size, embed_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a70dac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, block_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, embed_size, block_size, batch_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ffn = FFN(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd9b92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, block_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.head_size = int(embed_size/num_heads)\n",
    "        self.q_proj = nn.Linear(embed_size, self.head_size, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_size, self.head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(embed_size, self.head_size, bias=False)\n",
    "     \n",
    "        #self.register_buffer('mask',torch.tril(torch.ones(batch_size, block_size, block_size, device=device,requires_grad=False)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, L, d/H)\n",
    "        B, L, _ = x.shape\n",
    "        q, k ,v = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "        kt = k.transpose(-1,-2)\n",
    "        #attention_matrix = F.softmax(q@kt, dim=-1)\n",
    "        # (B, L, L)\n",
    "        #attention_matrix[self.mask == 0] = -torch.inf\n",
    "        att = q@kt/ (self.head_size**0.5)\n",
    "        # magic trick for double softmax      \n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        mask = torch.tril(torch.ones(B, L, L, device=device,requires_grad=False))\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        weights = F.softmax(att, dim=-1)\n",
    "        att_output = weights@v\n",
    "        #residual connection\n",
    "        return att_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a38b53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt = GPT(vocab_size, embed_size, block_size, num_blocks, num_heads)\n",
    "model_gpt = model_gpt.to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer_gpt = torch.optim.AdamW(model_gpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c98ef9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataloaderlite(train_data, block_size, batch_size, device, shuffle=True, tag=\"train\")\n",
    "val_loader = dataloaderlite(val_data, block_size, batch_size, device, shuffle=True, tag=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95e859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1786, val loss 4.1812\n",
      "step 200: train loss 2.4062, val loss 2.4067\n",
      "step 400: train loss 2.2887, val loss 2.3027\n",
      "step 600: train loss 2.2259, val loss 2.2397\n",
      "step 800: train loss 2.1744, val loss 2.2169\n",
      "step 1000: train loss 2.1231, val loss 2.1922\n",
      "step 1200: train loss 2.1078, val loss 2.1632\n",
      "step 1400: train loss 2.0687, val loss 2.1254\n",
      "step 1600: train loss 2.0483, val loss 2.1270\n",
      "step 1800: train loss 2.0173, val loss 2.1139\n",
      "step 2000: train loss 2.0200, val loss 2.1088\n",
      "step 2200: train loss 2.0024, val loss 2.1078\n",
      "step 2400: train loss 1.9952, val loss 2.0943\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = loss_estimation(model_gpt)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = train_loader.__next__()\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits_gpt, loss_gpt = model_gpt(xb, yb)\n",
    "    optimizer_gpt.zero_grad(set_to_none=True)\n",
    "    loss_gpt.backward()\n",
    "    optimizer_gpt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "302c7a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EMEMEMEREERBE:\n",
      "\n",
      "GPOLIABED:\n",
      "And wince botear intes\n",
      "\n",
      "If the a mareld and and sethent\n",
      "Ah af by fothe at\n",
      "Whe fadd belemss cenes sou swith tote cor and ben aphnot as fay st of sed to at bleeft bet to ne thend he my banges ime, swand ad fine ow;\n",
      "And, dithel thit me winll ine tor me hell hot\n",
      "My woul walldst in ashist amins.\n",
      "\n",
      "QUENGTE:\n",
      "Bost laid an athy the her hend ampet,\n",
      "Yon twead ame be nome ad samif be thadelses?\n",
      "By merded we herd st batt\n",
      "Youl he math sestint.\n",
      "Ande mesend ad tholeser tait ilemor;\n",
      "Ant ouch watht thee why me amese an to ald them.\n",
      "\n",
      "ARCKET:\n",
      "I deead a ind that,\n",
      "Wous hare fours he ment ad fit but wis baill ingh,\n",
      "Fare thalse tay;\n",
      "And stowe ids wasedsinend\n",
      "Hatll se blave anot on bamppoth.\n",
      "Toly bre int aptis\n",
      "\n",
      "If bords terir themenge ist shars,\n",
      "Wadod to but ay\n",
      "Buse for, affeend the sume ster thy pordsse\n",
      "Wes inse ard ott\n",
      "And wo mos thesth he thead thassthe,\n",
      "Ahest,\n",
      "We lith fre wateneth ail the tou hou meathlly thimames? I'ds fes feaincbede toor\n",
      "\n",
      "ABHARDWAUS:\n",
      "Yil bo fot we be wous of ano\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_context_ids = model_gpt.generate(context, max_new_tokens=1000, top_k=10)\n",
    "print(decode(generated_context_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8941f26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8385\n"
     ]
    }
   ],
   "source": [
    "num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_trainable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
